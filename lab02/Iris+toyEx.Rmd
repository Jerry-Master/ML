---
title: "iris"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(MASS)
library(rgl)
```


### Example 1.2: Comparison between PCA and FDA on 2D toy data (added one group)

Let `Sigma` be a 2x2 positive-definite symmetric matrix specifying the covariance matrix of two variables

```{r}
N <- 200
(Sigma <- matrix(data=c(2,1.3,1.3,1),nrow=2,ncol=2,byrow=TRUE))
```


These are the eigenvalues (should be strictly positive):
```{r}
eigen (Sigma, only.values=TRUE)$values
```

Let's create class 1 ('red' class)
```{r}
mean.1 <- matrix(c(2,0),nrow=2,ncol=1)                         
X.red <- mvrnorm(N, mu=mean.1, Sigma=Sigma)
```


Let's create class 2 ('green' class)

```{r}
mean.2 <- -mean.1    # notice the "-"
X.green <- mvrnorm(N, mu=mean.2, Sigma=Sigma)
```

Let's create class 3 ('blue' class)

```{r}
mean.3 <- matrix(c(-4,-3),nrow=2,ncol=1)    
X.blue <- mvrnorm(N, mu=mean.3, Sigma=Sigma)
```

```{r}
par(pty="s")
plot(c(X.red[,1],X.green[,1], X.blue[,1]), c(X.red[,2],X.green[,2],X.blue[,2]), 
     xlim=c(-8,8),ylim=c(-6,4),
     col=c(rep('red',N),rep('green',N),rep('blue',N)), main="Toy data", xlab="X1", ylab="X2")
```

Now we glue both classes one after the other and create a dataframe

```{r}
d <- data.frame(c(rep(1,N),rep(2,N),rep(3,N)), c(X.red[,1], X.green[,1], X.blue[,1]), c(X.red[,2], X.green[,2], X.blue[,2]))
colnames(d) <- c("target", "X1", "X2")
d$target <- as.factor(d$target)
summary(d)
```

#### Let's perform FDA

Call to FDA (also known as LDA, because there is a strong relationship with it, see a later lecture)
```{r}
myLDA <- lda(d[c(2,3)],d[,1])
```

Now we show the decision boundaries for the three classes.

```{r}
decisionplot <- function(model, data, predictors, class = NULL, predict_type = "class",
  resolution = 100, showgrid = TRUE, ...) {

  if(!is.null(class)) cl <- data[,class] else cl <- 1
  data <- data[,predictors]
  k <- length(unique(cl))

  plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)

  # make grid
  r <- sapply(data, range, na.rm = TRUE)
  xs <- seq(r[1,1], r[2,1], length.out = resolution)
  ys <- seq(r[1,2], r[2,2], length.out = resolution)
  g <- cbind(rep(xs, each=resolution), rep(ys, time = resolution))
  colnames(g) <- colnames(r)
  g <- as.data.frame(g)

  ### guess how to get class labels from predict
  ### (unfortunately not very consistent between models)
  p <- predict(model, g, type = predict_type)
  if(is.list(p)) p <- p$class
  p <- as.factor(p)

  if(showgrid) points(g, col = as.integer(p)+1L, pch = ".")

  z <- matrix(as.integer(p), nrow = resolution, byrow = TRUE)
  contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
    lwd = 2, levels = (1:(k-1))+.5)

  invisible(z)
}

decisionplot(myLDA, d, c(2,3), 1, resolution=100)
```

We can also compute the projections of the two classes

```{r}
myLDA.proj <- d[,2] * myLDA$scaling[1] + d[,3] * myLDA$scaling[2]

plot(myLDA.proj, c(rep(0,N),rep(0,N),rep(0,N)), col=c(rep('green',N),rep('red',N),rep('blue',N)),
     main='FDA projection as seen in 1D', xlab="Discriminant", ylab="")
```
We can change the axis in 2D:

```{r}
myLDA.proj <- as.matrix(d[c(2,3)]) %*% as.matrix(myLDA$scaling)

plot(myLDA.proj[,1], myLDA.proj[,2], col=c(rep('green',N),rep('red',N),rep('blue',N)),
     main='FDA change of axis', xlab="Discriminant", ylab="")
```

## Iris dataset

```{r}
library(MASS)
data(iris)

summary(iris)
```

```{r}
boxplot(iris)
```

```{r}
iris$col <- 'blue'
iris$col[iris$Species == 'setosa'] <- 'red'
iris$col[iris$Species == 'versicolor'] <- 'green'

head(iris)
levels(iris$Species)
```

FDA and PCA projections for the raw data:
```{r}
pca.raw <- prcomp(iris[c(1,2,3,4)])
lda.raw <- lda(iris[c(1,2,3,4)], iris[,5])

pca.proj1 <- as.matrix(iris[c(1,2,3,4)]) %*% as.matrix(pca.raw$rotation[,1:2])
lda.proj1 <- as.matrix(iris[c(1,2,3,4)]) %*% as.matrix(lda.raw$scaling)

plot(pca.proj1[,1], pca.proj1[,2], col=iris$col, main = 'PCA projection 2D', xlab="", ylab="")
plot(lda.proj1[,1], lda.proj1[,2], col=iris$col, main = 'LDA projection 2D', xlab="", ylab="")
```

FDA and PCA projections for the scaled data:
```{r}
dd <- iris
dd[c(1,2,3,4)] <- apply(iris[c(1,2,3,4)], 2, scale)

pca.scaled <- prcomp(dd[c(1,2,3,4)])
lda.scaled <- lda(dd[c(1,2,3,4)], dd[,5])

pca.proj2 <- as.matrix(iris[c(1,2,3,4)]) %*% as.matrix(pca.scaled$rotation[,1:2])
lda.proj2 <- as.matrix(iris[c(1,2,3,4)]) %*% as.matrix(lda.scaled$scaling)

plot(pca.proj2[,1], pca.proj2[,2], col=dd$col, main = 'PCA projection 2D', xlab="", ylab="")
plot(lda.proj2[,1], lda.proj2[,2], col=dd$col, main = 'LDA projection 2D', xlab="", ylab="")
```

A 3D plot with PCA

```{r}
loadings <- as.matrix(iris[c(1,2,3,4)]) %*% as.matrix(pca.scaled$rotation[,1:3])

# 3D scatterplot (can be rotated and zoomed with the mouse)
plot3d(loadings[,1], loadings[,2], loadings[,3], "PC1", "PC2", "PC3",
       size = 4, col=dd$col, main="Iris Data")

text3d(loadings[,1], loadings[,2], loadings[,3], color = "black", texts=rownames(iris), adj = c(0.85, 0.85), cex=0.6)
```

Con dos componentes ya se explica un 95% de la varianza, aÃ±adiendo una tercera permite visualizar casi perfectamente los datos. Con LDA no se puede representar en 3D porque solo hay tres grupos.

```{r}
summary(pca.scaled)
```



